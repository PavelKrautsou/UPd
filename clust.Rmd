Clustering
==========


In this file I will use real life dataset from UCI Machine Learning Repository. 
Dataset can be found following this link: https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling 
It is the real dataset about the students' knowledge status about the subject of Electrical DC Machines. The dataset had been obtained from Ph.D. Thesis.
Attribute Information:

STG (The degree of study time for goal object materails), (input value) 
SCG (The degree of repetition number of user for goal object materails) (input value) 
STR (The degree of study time of user for related objects with goal object) (input value) 
LPR (The exam performance of user for related objects with goal object) (input value) 
PEG (The exam performance of user for goal objects) (input value) 
UNS (The knowledge level of user) (target value) 



This data was originally used for classification, but it can be used as good toy example for clustering 
techniques. 

Following command reads data into R.
```{r}
data =read.table("UserKnowledge.csv", sep = ",", header = TRUE)
```

This somewhat cryptic command allows me to check for missing values. 
```{r}
apply((apply(data,2,is.na)),2,sum)
```
I use str() and summary() to have first glance into data. I also check column mean and standard deviations.

```{r}
str(data)
summary(data)
apply(data[,1:5],2,mean)
apply(data[,1:5],2,sd)
```

As we can see data is in such a way that all variables of interest have roughly the same mean and standard  
deviation. 
 
The last column of the table is a response variable, so it won't be used during clustering procedure. 
```{r}
dataClust = data[,-6]
```

K-Means Clustering
==================

We can perform K-means clustering in R using kmeans() function. After blatant data snooping that allowed myself
by learning that students had been already classified into 4 groups I chose to use 4 clusters.

```{r}
km.out=kmeans(dataClust,4,nstart=15)
km.out
```

Hierarchical Clustering
=======================
We will use these same data and use hierarchical clustering. I used Euclidean distance measure because all features are numeric and it is a default choice for numeric data. Choice of complete linkage method was based on the fact that it produced a better dendagram then other methods on this particular data.

```{r}
hc.complete=hclust(dist(dataClust),method="complete")
plot(hc.complete)
abline(h = 1.3, col = "red")
```

I used read line to show a point suitable for cutting dendagram into four distinct clusters.
```{r}
hc.cut=cutree(hc.complete,4)
```


Clustering doesn't capture the real division between students knowledge levels partially well. This match is far from perfect, but could have been useful, if we didn't know real levels. We can see the results in the tables below. It does not mater to which cluster observations assigned to as long as it is homogeneous group. 
In this case it's hard to make this claim. 

```{r}
table(hc.cut,data$UNS)
table(hc.cut,km.out$cluster)
table(km.out$cluster, data$UNS)
```



